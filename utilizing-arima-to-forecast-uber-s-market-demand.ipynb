{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67d1a83ea91b3a1c01cd1ab59449aabb5ee0ef76"
   },
   "source": [
    "# Uber market demand prediction using seasonal ARIMA with grid seach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "884f3d9b037a8efd9d8c61d3102680622bc7a229"
   },
   "source": [
    "Market demand plays a crucial part in the marketing strategy of any company. Forecasting such demand becomes crucial when the market is filled with competition, and a small mismatch in supply and demand can lead to a customer switching to another service provider.\n",
    "\n",
    "In this notebook we look at a classical algorithm(ARIMA) which can be used to predict the demand for user trips in the upcoming week for a particular location. Particularly, we will be utilizing Uber's 2014 user trips data of New York city, to accomplish the same.\n",
    "\n",
    "The dataset can be found on kaggle.com(https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "_uuid": "d0c9f5b82e46d0161486270c707dec5d529d220b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b54536ca7a04126b7ed0f4e4a7b762753f14426"
   },
   "source": [
    "First we upload our data-set into a single data-frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "_uuid": "a9f8c3f8e0ea3c7cd28adc9fdcc1392e7ff121f0"
   },
   "outputs": [],
   "source": [
    "#Preparing the uber 2014 main dataset\n",
    "def prepare_2014_df():\n",
    "    \n",
    "    #Loading datasets\n",
    "    uber_2014_apr=pd.read_csv('../input/uber-raw-data-apr14.csv',header=0)\n",
    "    uber_2014_may=pd.read_csv('../input/uber-raw-data-may14.csv',header=0)\n",
    "    uber_2014_jun=pd.read_csv('../input/uber-raw-data-jun14.csv',header=0)\n",
    "    uber_2014_jul=pd.read_csv('../input/uber-raw-data-jul14.csv',header=0)\n",
    "    uber_2014_aug=pd.read_csv('../input/uber-raw-data-aug14.csv',header=0)\n",
    "    uber_2014_sep=pd.read_csv('../input/uber-raw-data-sep14.csv',header=0)\n",
    "\n",
    "    \n",
    "    #Merging\n",
    "    df = uber_2014_apr.append([uber_2014_may,uber_2014_jun,uber_2014_jul,uber_2014_aug,uber_2014_sep], ignore_index=True)\n",
    "    \n",
    "    #returning merged dataframe\n",
    "    return df\n",
    "\n",
    "#Uber 2014 dataset\n",
    "uber_2014_master = prepare_2014_df()\n",
    "uber_2014_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70823a779b1fd01712f441eb8cff43356ff0cbf9"
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Next, we prepare the data-frame so that it is in a time-series format, which can then be utilized for modelling.\n",
    "Since we are only looking at a basic time-series forecast model, we will only be utilizing the Date/Time column for now.\n",
    "\n",
    "I plan to predict the demand at a day level and hence we will be resampling the data at a day level. However deping on the need, we can sample the data at different levels(Hour,Month,Year etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "_uuid": "dd954657e16ac4fd4178efbf4a6d14fe04202043"
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def create_day_series(df):\n",
    "    \n",
    "    # Grouping by Date/Time to calculate number of trips\n",
    "    day_df = pd.Series(df.groupby(['Date/Time']).size())\n",
    "    # setting Date/Time as index\n",
    "    day_df.index = pd.DatetimeIndex(day_df.index)\n",
    "    # Resampling to daily trips\n",
    "    day_df = day_df.resample('1D').apply(np.sum)\n",
    "    \n",
    "    return day_df\n",
    "\n",
    "day_df_2014 = create_day_series(uber_2014_master)\n",
    "day_df_2014.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f05e748476cf20c1d803e1da72884411a9c90582"
   },
   "source": [
    "Now that we have the time-series data-frame ready, we can look into some initial visualizations of the data to decide our parameters for the ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "_uuid": "03ce10e897b9120e617d1ef704fa803f10e970e5"
   },
   "outputs": [],
   "source": [
    "#Checking trend and autocorrelation\n",
    "def initial_plots(time_series, num_lag):\n",
    "\n",
    "    #Original timeseries plot\n",
    "    plt.figure(1)\n",
    "    plt.plot(time_series)\n",
    "    plt.title('Original data across time')\n",
    "    plt.figure(2)\n",
    "    plot_acf(time_series, lags = num_lag)\n",
    "    plt.title('Autocorrelation plot')\n",
    "    plot_pacf(time_series, lags = num_lag)\n",
    "    plt.title('Partial autocorrelation plot')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#Augmented Dickey-Fuller test for stationarity\n",
    "#checking p-value\n",
    "print('p-value: {}'.format(adfuller(day_df_2014)[1]))\n",
    "\n",
    "#plotting\n",
    "initial_plots(day_df_2014, 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bbe7ddeb1faebf9734bf616de1bafd6e8ff36157"
   },
   "source": [
    "Looking at the ADF test we see that clearly the time-series is not stationary(p-value>0.05 i.e for a confidence level of 95%), hence differencing is required.\n",
    "\n",
    "Before we even analyse the ACF and PACF plots we need to difference and test for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "_uuid": "34efd86d6482acd7c3ce592799d82d2bd1ec818f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#storing differenced series\n",
    "diff_series = day_df_2014.diff(periods=1)\n",
    "\n",
    "#Augmented Dickey-Fuller test for stationarity\n",
    "#checking p-value\n",
    "print('p-value: {}'.format(adfuller(diff_series.dropna())[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06aba98d36c6f3f12c23ae00d026477ac365f211"
   },
   "source": [
    "Looks like the series is stationary now (as p-value < 0.05, we can assume stationarity with a confidence level of 95%, even higher actually). So a differencing of 1 should be perfect!\n",
    "\n",
    "Now lets look at the ACF and PACF plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "_uuid": "a24b65f41bf0282f2b0976f45c42967283f81015"
   },
   "outputs": [],
   "source": [
    "initial_plots(diff_series.dropna(), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9fe953d178a3d711e335010145d8360b7e3f98b"
   },
   "source": [
    "From the ACF and PACF plots we can see a clear spike at every 7 day interval. And since this appears clearly in the ACF plot this whows a seasonal MA component of 1.\n",
    "\n",
    "## Fitting SARIMAX models\n",
    "\n",
    "Although at this point the components can be guessed at ARIMA(0,1,0)(0,0,1)[7], we will implement a grid search to find the best fitting values, using RMSE as the deciding factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "_uuid": "6354c979f7faf950904d8bc9f876fca34d8f4b90",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining RMSE\n",
    "def rmse(x,y):\n",
    "    return sqrt(mean_squared_error(x,y))\n",
    "\n",
    "#fitting ARIMA model on dataset\n",
    "def SARIMAX_call(time_series,p_list,d_list,q_list,P_list,D_list,Q_list,s_list,test_period):    \n",
    "    \n",
    "    #Splitting into training and testing\n",
    "    training_ts = time_series[:-test_period]\n",
    "    \n",
    "    testing_ts = time_series[len(time_series)-test_period:]\n",
    "    \n",
    "    error_table = pd.DataFrame(columns = ['p','d','q','P','D','Q','s','AIC','BIC','RMSE'],\\\n",
    "                                                           index = range(len(ns_ar)*len(ns_diff)*len(ns_ma)*len(s_ar)\\\n",
    "                                                                         *len(s_diff)*len(s_ma)*len(s_list)))\n",
    "    count = 0\n",
    "    \n",
    "    for p in p_list:\n",
    "        for d in d_list:\n",
    "            for q in q_list:\n",
    "                for P in P_list:\n",
    "                    for D in D_list:\n",
    "                        for Q in Q_list:\n",
    "                            for s in s_list:\n",
    "                                #fitting the model\n",
    "                                SARIMAX_model = SARIMAX(training_ts.astype(float),\\\n",
    "                                                        order=(p,d,q),\\\n",
    "                                                        seasonal_order=(P,D,Q,s),\\\n",
    "                                                        enforce_invertibility=False)\n",
    "                                SARIMAX_model_fit = SARIMAX_model.fit(disp=0)\n",
    "                                AIC = np.round(SARIMAX_model_fit.aic,2)\n",
    "                                BIC = np.round(SARIMAX_model_fit.bic,2)\n",
    "                                predictions = SARIMAX_model_fit.forecast(steps=test_period,typ='levels')\n",
    "                                RMSE = rmse(testing_ts.values,predictions.values)                                \n",
    "\n",
    "                                #populating error table\n",
    "                                error_table['p'][count] = p\n",
    "                                error_table['d'][count] = d\n",
    "                                error_table['q'][count] = q\n",
    "                                error_table['P'][count] = P\n",
    "                                error_table['D'][count] = D\n",
    "                                error_table['Q'][count] = Q\n",
    "                                error_table['s'][count] = s\n",
    "                                error_table['AIC'][count] = AIC\n",
    "                                error_table['BIC'][count] = BIC\n",
    "                                error_table['RMSE'][count] = RMSE\n",
    "                                \n",
    "                                count+=1 #incrementing count        \n",
    "    \n",
    "    #returning the fitted model and values\n",
    "    return error_table\n",
    "\n",
    "ns_ar = [0,1,2]\n",
    "ns_diff = [1]\n",
    "ns_ma = [0,1,2]\n",
    "s_ar = [0,1]\n",
    "s_diff = [0,1] \n",
    "s_ma = [1,2]\n",
    "s_list = [7]\n",
    "\n",
    "error_table = SARIMAX_call(day_df_2014,ns_ar,ns_diff,ns_ma,s_ar,s_diff,s_ma,s_list,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fa79736e9543e2d9d42f16e91ce75725ec413fe"
   },
   "source": [
    "Now that we have obtained the RMSE values for different combinations, we can take a look at the lowest 5 RMSE values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "_uuid": "1ee1001a90ef9168bb39e98e1dcee1d8bcc41cf6"
   },
   "outputs": [],
   "source": [
    "# printing top 5 lowest RMSE from error table\n",
    "error_table.sort_values(by='RMSE').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "712c6b7ed3ebb16aa31af97eafdefec0a02c08bd"
   },
   "source": [
    "We see that ARIMA(0,1,0)(1,0,2)[7] gives the lowest RMSE. However, next best RMSE is of ARIMA(0,1,0)(0,1,2)[7] has lower AIC and BIC scores, which tells us that it fits the data much better than the lowest RMSE ARIMA fit.\n",
    "\n",
    "Here we can chose the second ARIMA if we want a more robust solution, which can generalize well to other situations as well, however this ensures that we do not get the best RMSE for this testing data.\n",
    "\n",
    "## Forecasting\n",
    "\n",
    "I will forecast using ARIMA(0,1,0)(0,1,2)[7] as I want the model to fit the data better along with giving me a lower RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "_uuid": "2744048e60b08abb2d63abc722ff46dff4701dbe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predicting values using the fitted model\n",
    "def predict(time_series,p,d,q,P,D,Q,s,n_days,conf):\n",
    "    \n",
    "    #Splitting into training and testing\n",
    "    training_ts = time_series[:-n_days]\n",
    "    \n",
    "    testing_ts = time_series[len(time_series)-n_days:]\n",
    "    \n",
    "    #fitting the model\n",
    "    SARIMAX_model = SARIMAX(training_ts.astype(float),\\\n",
    "                            order=(p,d,q),\\\n",
    "                            seasonal_order=(P,D,Q,s),\\\n",
    "                            enforce_invertibility=False)\n",
    "    SARIMAX_model_fit = SARIMAX_model.fit(disp=0)\n",
    "    \n",
    "    #Predicting\n",
    "    SARIMAX_prediction = pd.DataFrame(SARIMAX_model_fit.forecast(steps=n_days,alpha=(1-conf)).values,\\\n",
    "                          columns=['Prediction'])\n",
    "    SARIMAX_prediction.index = pd.date_range(training_ts.index.max()+1,periods=n_days)\n",
    "    \n",
    "    #Plotting\n",
    "    plt.figure(4)\n",
    "    plt.title('Plot of original data and predicted values using the ARIMA model')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Number of Trips')\n",
    "    plt.plot(time_series[1:],'k-', label='Original data')\n",
    "    plt.plot(SARIMAX_prediction,'b--', label='Next {}days predicted values'.format(n_days))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #Returning predicitons\n",
    "    return SARIMAX_prediction\n",
    "\n",
    "#Predicting the values and builing an 80% confidence interval\n",
    "prediction = predict(day_df_2014,0,1,0,0,1,2,7,7,0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1d22c2bea6edcbc8b0b5978be10eb2bccfc2f07"
   },
   "source": [
    "Thus using this simple classical Seasonal ARIMA model we can utilize the forecasted data to plan and alter our marketing strategy for the upcoming week(or any timeframe upon which predictions are to be made) in any location.\n",
    "\n",
    "In this example, I have just used the complete dataset to forecast the demand, however we need not restrict ourselves to this level. We can split the data-set based on location and obtain forecasts for different pick-up zones in NYC(which would add even more value towards our predictions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
